{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1.关系运算符构成的表达式的类型为布尔型：True or False\n",
    "\n",
    "2.逻辑运算符and,or构造的表达式的类型取决于运算数和其短路特性\n",
    "* and: 若左操作数为真，返回右操作数；否则返回左操作数\n",
    "* or: 若左操作数为真，返回右操作数；否则返回右操作数\n",
    "* 应用场景：常用于条件赋值和简化代码逻辑"
   ],
   "id": "5683c92c46ed15b4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T08:22:59.489886Z",
     "start_time": "2025-04-16T08:22:59.480412Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(0 and 1)\n",
    "print(\"\" or \"abc\")\n",
    "print(1 and [])"
   ],
   "id": "2ba2f969667089e9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "abc\n",
      "[]\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "3.not会先将操作数转化为布尔值，再取反\n",
    "* 假值包括：False, None, 0, 空字符串, 空容器\n",
    "* 其他均视为真值"
   ],
   "id": "7ca2b771d70587e9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T08:29:27.466002Z",
     "start_time": "2025-04-16T08:29:27.457032Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(not 0)  # True(0为假值)\n",
    "print(not \"abc\")  # False(非空字符串为真值)"
   ],
   "id": "90e0423c669ec24e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 字典常用方法: items(), keys(), values()",
   "id": "56c0c33a41e21cdb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T08:51:45.221168Z",
     "start_time": "2025-04-16T08:51:45.211315Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llms1 = {'DeepSeek': '深度求索', 'ChatGPT 4.5': 'OpenAI'}\n",
    "# items()\n",
    "print(llms1.items())\n",
    "print(type(llms1.items()))\n",
    "print(tuple(llms1.items()))\n",
    "for k, v in llms1.items():\n",
    "    print(f'{k}: {v}')\n",
    "\n",
    "# keys()\n",
    "print()\n",
    "print(llms1.keys())\n",
    "print(type(llms1.keys()))\n",
    "print(tuple(llms1.keys()))\n",
    "for k in llms1.keys():\n",
    "    print(f'{k}: {llms1[k]}')\n",
    "\n",
    "# values()\n",
    "print()\n",
    "llms2 = {'DeepSeek': '深度求索', 'ChatGPT 4.5': 'OpenAI', 'Qwen2.0': '阿里', 'Qwen2.5': '阿里'}\n",
    "print(llms2.values())\n"
   ],
   "id": "5363bbca35aaefe3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('DeepSeek', '深度求索'), ('ChatGPT 4.5', 'OpenAI')])\n",
      "<class 'dict_items'>\n",
      "(('DeepSeek', '深度求索'), ('ChatGPT 4.5', 'OpenAI'))\n",
      "DeepSeek: 深度求索\n",
      "ChatGPT 4.5: OpenAI\n",
      "\n",
      "dict_keys(['DeepSeek', 'ChatGPT 4.5'])\n",
      "<class 'dict_keys'>\n",
      "('DeepSeek', 'ChatGPT 4.5')\n",
      "DeepSeek: 深度求索\n",
      "ChatGPT 4.5: OpenAI\n",
      "\n",
      "dict_values(['深度求索', 'OpenAI', '阿里', '阿里'])\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "字典推导式\n",
    "特点：不修改元字典，创建新字典，大字典需谨慎"
   ],
   "id": "e98071424f819d35"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T09:05:35.500590Z",
     "start_time": "2025-04-16T09:05:35.494349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "word_frequency = {\n",
    "    'DeepSeek': 1,\n",
    "    'of': 10,\n",
    "    'China': 3, 'USA': 3, 'tariff': 3,  # 关税\n",
    "    'OpenAI': 1,\n",
    "    'the': 10, 'a': 20, 'an': 20\n",
    "}\n",
    "\n",
    "stop_words = {'of', 'the', 'a', 'an'}\n",
    "\n",
    "# 使用字典推导式过滤停用词\n",
    "filtered_word_frequency = {word: freq for word, freq in word_frequency.items() if word not in stop_words}\n",
    "\n",
    "print(filtered_word_frequency)"
   ],
   "id": "e56fac694c35140a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DeepSeek': 1, 'China': 3, 'USA': 3, 'tariff': 3, 'OpenAI': 1}\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "经典字典应用示例：\n",
    "* 词频率统计v0"
   ],
   "id": "92d734a0f6ebc33f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T09:09:25.603042Z",
     "start_time": "2025-04-16T09:09:25.594801Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def count_word_frequency(text):\n",
    "    words = text.split()\n",
    "    frequency = {}  # 创建空字典\n",
    "    for word in words:\n",
    "        # 若单词不存在，get返回0后加1；否则直接递增\n",
    "        frequency[word] = frequency.get(word, 0) + 1\n",
    "    return frequency\n",
    "\n",
    "\n",
    "text = \"apple banana orange apple banana apple\"\n",
    "result = count_word_frequency(text)\n",
    "print(result)"
   ],
   "id": "2c801795028b6626",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'apple': 3, 'banana': 2, 'orange': 1}\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* 词频率v1:\n",
    "去除常用词 给出出现频率最高的五个词和最低的五个词"
   ],
   "id": "c6c4064e7fd75394"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def word_frequency_analysis(text, stop_words=None):\n",
    "    \"\"\"统计词频，过滤停用词，返回最高和最低频率的词\"\"\"\n",
    "    # 1.预处理文本：转小写，去标点，分割单词\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    words = text.split()\n",
    "\n",
    "    if stop_words is None:\n",
    "        stop_words = {'the', 'of', 'with', 'a', 'an', 'in', 'is', 'it', 'for', 'and'}\n",
    "\n",
    "    # 2.过滤停用词\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # 3.统计词频\n",
    "    word_counts = Counter(filtered_words)\n",
    "\n",
    "    # 4.获取最高频的5个词\n",
    "    most_common = word_counts.most_common(5)\n",
    "\n",
    "    # 5.获取最低频的5个词\n",
    "    # 先按频率升序排序，再取前5个\n",
    "    least_common = sorted(word_counts.items(), key=lambda x: x[1])[:5]\n",
    "\n",
    "    return {\n",
    "        'most_common': most_common,\n",
    "        'least_common': least_common,\n",
    "        'total_words': len(filtered_words),\n",
    "        'unique_words': len(word_counts)\n",
    "    }"
   ],
   "id": "a2b4f011f08d0442"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "* 词频率v2：英文词频统计，去除停用词（最高最低）",
   "id": "b70011b59730f9da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import string\n",
    "\n",
    "\n",
    "def word_frequency_analysis(text, stop_words=None):\n",
    "    \"\"\"统计词频，过滤停用词，返回最高和最低频率的词\"\"\"\n",
    "    # 1. 预处理文本：转小写，去标点\n",
    "    text = text.lower()\n",
    "    # 删除所有标点符号\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # print(text)\n",
    "    # 分割单词\n",
    "    words = text.split()\n",
    "\n",
    "    # 2. 过滤停用词（若未提供，使用默认列表）\n",
    "    if stop_words is None:\n",
    "        stop_words = {'the', 'and', 'of', 'to', 'a', 'in', 'is', 'it', 'for', 'with'}\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # 3. 统计词频\n",
    "    frequency = {}\n",
    "    for word in filtered_words:\n",
    "        frequency[word] = frequency.get(word, 0) + 1\n",
    "\n",
    "    # 4. 排序并获取最高和最低频率词\n",
    "    # 按频率降序排序\n",
    "    sorted_freq = sorted(frequency.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_5 = sorted_freq[:5]\n",
    "    # 按频率升序排序\n",
    "    sorted_low = sorted(frequency.items(), key=lambda x: x[1])\n",
    "    bottom_5 = sorted_low[:5]\n",
    "\n",
    "    return top_5, bottom_5\n",
    "\n",
    "\n",
    "# 示例文本\n",
    "text = '''Python is a powerful language.\n",
    "    Python is easy to learn.\n",
    "    Python is widely used in data science and web development.\n",
    "    Python and data science are great together.\n",
    "    '''\n",
    "# 调用函数\n",
    "top, bottom = word_frequency_analysis(text)\n",
    "print(\"最高频率词:\", top)\n",
    "print(\"最低频率词:\", bottom)"
   ],
   "id": "b59d07d8e50dc1fe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "* 词频率v3：词云展示",
   "id": "c931a0d36ed1f30d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T12:27:18.165735Z",
     "start_time": "2025-04-16T12:27:18.149162Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成词云时出错: cannot open resource\n"
     ]
    }
   ],
   "execution_count": 2,
   "source": [
    "import jieba\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "\n",
    "def chinese_word_cloud(text, stop_words=None, font_path='SimHei.ttf',\n",
    "                       output_file=None, max_words=200, width=800,\n",
    "                       height=400, background_color='white',\n",
    "                       colormap='viridis', contour_color=None,\n",
    "                       contour_width=0):\n",
    "    \"\"\"\n",
    "    生成中文词云并返回词频统计\n",
    "\n",
    "    参数:\n",
    "        text (str): 要分析的文本\n",
    "        stop_words (set): 停用词集合\n",
    "        font_path (str): 字体文件路径\n",
    "        output_file (str): 词云图片保存路径\n",
    "        max_words (int): 显示的最大词数\n",
    "        width (int): 词云宽度\n",
    "        height (int): 词云高度\n",
    "        background_color (str): 背景颜色\n",
    "        colormap (str): 颜色映射\n",
    "        contour_color (str): 轮廓颜色\n",
    "        contour_width (int): 轮廓宽度\n",
    "\n",
    "    返回:\n",
    "        Counter: 词频统计结果\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 中文分词\n",
    "        words = jieba.lcut(text)\n",
    "\n",
    "        # 设置默认停用词\n",
    "        default_stopwords = {'的', '和', '是', '在', '了', '与', '及', '等', '就', '都', '而', '也', '又'}\n",
    "        stop_words = stop_words or default_stopwords\n",
    "\n",
    "        # 过滤停用词和单字\n",
    "        filtered_words = [word for word in words\n",
    "                          if word not in stop_words\n",
    "                          and len(word) > 1\n",
    "                          and not word.isspace()]\n",
    "\n",
    "        # 统计词频\n",
    "        frequency = Counter(filtered_words)\n",
    "\n",
    "        # 生成词云\n",
    "        wc = WordCloud(\n",
    "            font_path=font_path,\n",
    "            width=width,\n",
    "            height=height,\n",
    "            background_color=background_color,\n",
    "            max_words=max_words,\n",
    "            colormap=colormap,\n",
    "            contour_color=contour_color,\n",
    "            contour_width=contour_width\n",
    "        )\n",
    "        wc.generate_from_frequencies(frequency)\n",
    "\n",
    "        # 显示词云\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.imshow(wc, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "\n",
    "        # 保存词云图片\n",
    "        if output_file:\n",
    "            # 确保目录存在\n",
    "            os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "            wc.to_file(output_file)\n",
    "            print(f\"词云图片已保存到: {output_file}\")\n",
    "\n",
    "        plt.show()\n",
    "        return frequency\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"生成词云时出错: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# 文本示例，2025年4月份，最新AI资讯\n",
    "text = '''中国AI市场竞争呈现新态势，\n",
    "百度宣布自4月1日起全面免费开放Ernie Bot服务，\n",
    "直接回应DeepSeek等新兴企业的挑战。\n",
    "数据显示，DeepSeek的R1模型在多项基准测试中表现接近GPT-4，\n",
    "但运营成本仅为同类产品的三分之一。\n",
    "这种技术突破与成本优势的组合，正在动摇OpenAI在中国市场的先发优势。\n",
    "值得关注的是，字节跳动旗下Doubao聊天机器人凭借短视频平台的流量优势，\n",
    "用户增长率持续领先传统搜索引擎衍生产品。\n",
    "'''\n",
    "\n",
    "# 自定义停用词\n",
    "custom_stopwords = {'的', '和', '是', '在', '了', '与', '等'}\n",
    "\n",
    "# 调用函数获取词频统计数据\n",
    "chinese_freq = chinese_word_cloud(\n",
    "    text,\n",
    "    stop_words=custom_stopwords,\n",
    "    font_path='SimHei.ttf',\n",
    "    output_file='wordcloud.png',\n",
    "    max_words=100,\n",
    "    width=1000,\n",
    "    height=600,\n",
    "    background_color='white',\n",
    "    colormap='plasma',\n",
    "    contour_color='steelblue',\n",
    "    contour_width=3\n",
    ")\n",
    "\n",
    "# 打印词频统计\n",
    "if chinese_freq:\n",
    "    print(\"\\n词频统计(前10):\")\n",
    "    for word, freq in chinese_freq.most_common(10):\n",
    "        print(f\"{word}: {freq}\")"
   ],
   "id": "24e49971d1240ba4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Python 的 `str.translate()` 方法和 `str.maketrans()` 函数\n",
    "\n",
    "## **`string.punctuation`**\n",
    "\n",
    "`string.punctuation` 是 Python 标准库 `string` 中的一个字符串常量，包含所有常见的英文标点符号：\n",
    "\n",
    "```python\n",
    "import string\n",
    "print(string.punctuation)  # 输出：!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **`str.maketrans(x, y, z)`**\n",
    "\n",
    "`str.maketrans()` 用于创建一个字符映射表（翻译表），它有三种调用方式：\n",
    "\n",
    "- **`str.maketrans(x, y, z)`**\n",
    "  其中：\n",
    "  - `x` 和 `y` 是等长的字符串，表示将 `x` 中的字符替换为 `y` 中的对应字符。\n",
    "  - `z` 是一个字符串，表示需要删除的字符。\n",
    "\n",
    "在代码中：\n",
    "\n",
    "```python\n",
    "str.maketrans('', '', string.punctuation)\n",
    "```\n",
    "\n",
    "- 前两个参数是空字符串 `''`，表示 **不需要替换任何字符**。\n",
    "- 第三个参数是 `string.punctuation`，表示 **所有标点符号都需要被删除**。\n",
    "\n",
    "生成的映射表会告诉 `translate()` 方法：“把标点符号全部删除”。\n",
    "\n",
    "---\n",
    "\n",
    "## **`text.translate(translation_table)`**\n",
    "\n",
    "`str.translate()` 方法根据映射表对字符串进行字符替换或删除：\n",
    "\n",
    "- 传入 `str.maketrans('', '', string.punctuation)` 生成的映射表后，它会 **直接删除 `text` 中所有出现在 `string.punctuation` 中的标点符号**。\n",
    "\n",
    "---\n",
    "\n",
    "## **完整示例**\n",
    "\n",
    "```python\n",
    "import string\n",
    "\n",
    "text = \"Hello, World! This is an example—with some punctuation.\"\n",
    "cleaned_text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "print(cleaned_text)  # 输出：Hello World This is an examplewith some punctuation\n",
    "```\n",
    "\n",
    "**输出结果**：\n",
    "\n",
    "```tex\n",
    "Hello World This is an examplewith some punctuation\n",
    "```\n",
    "\n",
    "所有标点符号（`, ! — .`）都被删除了。\n",
    "\n",
    "---\n",
    "\n",
    "## **注意事项**\n",
    "\n",
    "1. **仅针对 ASCII 标点**\n",
    "   `string.punctuation` 只包含英文标点，如果需要处理其他语言（如中文标点），需自定义要删除的字符集，例如：\n",
    "\n",
    "   ```python\n",
    "   chinese_punctuation = '，。！？；：“”‘’（）【】《》…—'\n",
    "   text.translate(str.maketrans('', '', chinese_punctuation))\n",
    "   ```\n",
    "\n",
    "2. **性能高效**\n",
    "   `translate()` 是基于 C 实现的，比用正则表达式（如 `re.sub(r'[^\\w\\s]', '', text)`）或循环遍历字符更快。\n",
    "\n",
    "3. **保留空格和字母数字**\n",
    "   此方法不会删除空格或字母数字字符（`a-z, A-Z, 0-9`），仅删除标点。\n",
    "\n",
    "---\n",
    "\n",
    "## 总结\n",
    "\n",
    "这行代码通过 `str.maketrans()` 生成一个删除标点的映射表，再用 `translate()` 快速清理文本，是 Python 中高效处理标点符号的标准方法。"
   ],
   "id": "3813ce397be716a7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **常见的英文停用词列表**\n",
    "以下是一些常用的英文停用词（部分来自 `nltk` 和 `sklearn` 的标准停用词列表）：\n",
    "\n",
    "```python\n",
    "english_stop_words = {\n",
    "    'a', 'an', 'the',  # 冠词\n",
    "    'and', 'or', 'but', 'if', 'because',  # 连词\n",
    "    'of', 'to', 'in', 'for', 'on', 'with', 'at', 'by', 'from', 'up', 'down', 'about', 'into', 'over', 'under',  # 介词\n",
    "    'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them',  # 代词\n",
    "    'my', 'your', 'his', 'its', 'our', 'their',  # 物主代词\n",
    "    'this', 'that', 'these', 'those',  # 指示代词\n",
    "    'is', 'am', 'are', 'was', 'were', 'be', 'been', 'being',  # be动词\n",
    "    'have', 'has', 'had', 'having',  # have动词\n",
    "    'do', 'does', 'did', 'doing',  # do动词\n",
    "    'can', 'could', 'will', 'would', 'shall', 'should', 'may', 'might', 'must',  # 情态动词\n",
    "    'not', 'no', 'yes',  # 否定词\n",
    "    'here', 'there', 'when', 'where', 'why', 'how',  # 疑问词/副词\n",
    "    'some', 'any', 'all', 'both', 'each', 'few', 'many', 'most', 'other', 'such',  # 限定词\n",
    "    'only', 'just', 'also', 'very', 'too', 'so', 'than', 'then', 'again', 'more', 'less',  # 副词\n",
    "    'as', 'like', 'same', 'different',  # 比较词\n",
    "    'what', 'which', 'who', 'whom', 'whose',  # 疑问代词\n",
    "    'one', 'ones', 'every', 'none', 'nothing', 'something', 'anything', 'everything',  # 不定代词\n",
    "    'get', 'got', 'getting', 'go', 'went', 'gone', 'going',  # 常见动词\n",
    "    'say', 'said', 'saying', 'see', 'saw', 'seen', 'seeing',  # 常见动词\n",
    "    'make', 'made', 'making', 'take', 'took', 'taken', 'taking',  # 常见动词\n",
    "    'use', 'used', 'using', 'find', 'found', 'finding',  # 常见动词\n",
    "    'want', 'wanted', 'wanting', 'need', 'needed', 'needing',  # 常见动词\n",
    "    'good', 'bad', 'better', 'best', 'worse', 'worst',  # 形容词\n",
    "    'big', 'small', 'large', 'little', 'long', 'short',  # 形容词\n",
    "    'first', 'last', 'next', 'previous', 'early', 'late',  # 序数词/时间词\n",
    "    'well', 'better', 'best', 'badly', 'worse', 'worst',  # 副词\n",
    "    'now', 'then', 'today', 'yesterday', 'tomorrow', 'soon', 'later',  # 时间副词\n",
    "    'always', 'never', 'often', 'sometimes', 'usually', 'rarely',  # 频率副词\n",
    "    'very', 'quite', 'rather', 'too', 'enough',  # 程度副词\n",
    "    'also', 'either', 'neither',  # 连接副词\n",
    "    'perhaps', 'maybe', 'probably', 'certainly',  # 模态副词\n",
    "                       }\n"
   ],
   "id": "823b7a14d1640a50"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
